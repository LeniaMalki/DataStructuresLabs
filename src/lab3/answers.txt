Running the slow program
------------------------

Q: What is the complexity of findSimilarity?
Answer in terms of N, the total number of 5-grams in the input files.
Assume that the number of 5-grams that occur in more than one file is a small
constant - that is, there is not much plagiarised text.

A: 

Q: How long did the program take on the 'small' and 'medium' directories?
Is the ratio between the times what you would expect, given the complexity?
Explain very briefly why.

Small

Reading all input files took 0.07 seconds
Building n-gram index took 0.00 seconds
Computing similarity scores took 2.45 seconds
Finding the most similar files took 0.00 seconds
In total the program took 2.52 seconds

Medium

Reading all input files took 0.29 seconds
Building n-gram index took 0.00 seconds
Computing similarity scores took 469.16 seconds
Finding the most similar files took 0.07 seconds
In total the program took 469.52 seconds


A:

Q: How long do you predict the program would take to run on the 'huge'
directory?

A:

Using binary search trees
-------------------------

Q: Which of the trees usually become unbalanced?

A: Files is very unbalanced. Index is also unbalanced.

Q (optional): Is there a simple way to stop these trees becoming unbalanced?

A (optional): 

Using scapegoat trees
---------------------

Q: Now what is the total complexity of buildIndex and findSimilarity?
Again, assume a total of N 5-grams, and a constant number of 5-grams that
occur in more than one file.

A:

Q (optional): What if the total similarity score is an arbitrary number S,
rather than a small constant?

A (optional):

Q (optional): Did you find any text that was clearly copied?

A (optional):
